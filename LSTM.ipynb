{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generative Models for Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) LSTM: Train an LSTM to mimic Russell’s style and thoughts:\n",
    "\n",
    "i. Concatenate your text files to create a corpus of Russell’s writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow \n",
    "import keras\n",
    "import numpy as np \n",
    "#import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"corpus.txt\"\n",
    "raw_text = open(filename,encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Use a character-level representation for this model by using extended ASCII that has N = 256 characters. Each character will be encoded into a an integer using its ASCII code. Rescale the integers to the range [0, 1], because LSTM uses a sigmoid activation function. LSTM will receive the rescaled integers as its input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1577211\n",
      "Total Vocab:  99\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Choose a window size, e.g., W = 100.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  1577111\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100 #window size\n",
    "d_X = []\n",
    "d_Y = []\n",
    "for i in range(0, n_chars - seq_length, 1): #adding 100 characters to seq_in\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    d_X.append([char_to_int[char] for char in seq_in])\n",
    "    d_Y.append(char_to_int[seq_out])\n",
    "n_patterns = len(d_X)\n",
    "print(\"Total Patterns: \", n_patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(d_X, (n_patterns, seq_length, 1))\n",
    "# normalize the data\n",
    "X = X / float(n_vocab)\n",
    "# output variable- one hot encode\n",
    "y = np_utils.to_categorical(d_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model- with one hidden layer and dropout of 20%\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### choosing 20 epoch and a batch size of 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 2.7348Epoch 00001: loss improved from inf to 2.73484, saving model to weights-improvement-01-2.7348.hdf5\n",
      "1577111/1577111 [==============================] - 4055s 3ms/step - loss: 2.7348\n",
      "Epoch 2/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 2.4442Epoch 00002: loss improved from 2.73484 to 2.44423, saving model to weights-improvement-02-2.4442.hdf5\n",
      "1577111/1577111 [==============================] - 3905s 2ms/step - loss: 2.4442\n",
      "Epoch 3/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 2.2658Epoch 00003: loss improved from 2.44423 to 2.26580, saving model to weights-improvement-03-2.2658.hdf5\n",
      "1577111/1577111 [==============================] - 3906s 2ms/step - loss: 2.2658\n",
      "Epoch 4/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 2.1488Epoch 00004: loss improved from 2.26580 to 2.14877, saving model to weights-improvement-04-2.1488.hdf5\n",
      "1577111/1577111 [==============================] - 3909s 2ms/step - loss: 2.1488\n",
      "Epoch 5/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 2.0687Epoch 00005: loss improved from 2.14877 to 2.06870, saving model to weights-improvement-05-2.0687.hdf5\n",
      "1577111/1577111 [==============================] - 3908s 2ms/step - loss: 2.0687\n",
      "Epoch 6/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 2.0103Epoch 00006: loss improved from 2.06870 to 2.01030, saving model to weights-improvement-06-2.0103.hdf5\n",
      "1577111/1577111 [==============================] - 3909s 2ms/step - loss: 2.0103\n",
      "Epoch 7/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 1.9653Epoch 00007: loss improved from 2.01030 to 1.96533, saving model to weights-improvement-07-1.9653.hdf5\n",
      "1577111/1577111 [==============================] - 4026s 3ms/step - loss: 1.9653\n",
      "Epoch 8/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 1.9295Epoch 00008: loss improved from 1.96533 to 1.92948, saving model to weights-improvement-08-1.9295.hdf5\n",
      "1577111/1577111 [==============================] - 4006s 3ms/step - loss: 1.9295\n",
      "Epoch 9/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 1.9010Epoch 00009: loss improved from 1.92948 to 1.90102, saving model to weights-improvement-09-1.9010.hdf5\n",
      "1577111/1577111 [==============================] - 4045s 3ms/step - loss: 1.9010\n",
      "Epoch 10/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 1.8763Epoch 00010: loss improved from 1.90102 to 1.87627, saving model to weights-improvement-10-1.8763.hdf5\n",
      "1577111/1577111 [==============================] - 3954s 3ms/step - loss: 1.8763\n",
      "Epoch 11/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 1.8756Epoch 00011: loss improved from 1.87627 to 1.87562, saving model to weights-improvement-11-1.8756.hdf5\n",
      "1577111/1577111 [==============================] - 3943s 3ms/step - loss: 1.8756\n",
      "Epoch 12/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 1.8468Epoch 00012: loss improved from 1.87562 to 1.84679, saving model to weights-improvement-12-1.8468.hdf5\n",
      "1577111/1577111 [==============================] - 3970s 3ms/step - loss: 1.8468\n",
      "Epoch 13/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 1.8442Epoch 00013: loss improved from 1.84679 to 1.84422, saving model to weights-improvement-13-1.8442.hdf5\n",
      "1577111/1577111 [==============================] - 3937s 2ms/step - loss: 1.8442\n",
      "Epoch 14/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 1.8257Epoch 00014: loss improved from 1.84422 to 1.82568, saving model to weights-improvement-14-1.8257.hdf5\n",
      "1577111/1577111 [==============================] - 3902s 2ms/step - loss: 1.8257\n",
      "Epoch 15/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 1.8094Epoch 00015: loss improved from 1.82568 to 1.80945, saving model to weights-improvement-15-1.8094.hdf5\n",
      "1577111/1577111 [==============================] - 3927s 2ms/step - loss: 1.8094\n",
      "Epoch 16/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 1.7965Epoch 00016: loss improved from 1.80945 to 1.79655, saving model to weights-improvement-16-1.7966.hdf5\n",
      "1577111/1577111 [==============================] - 3905s 2ms/step - loss: 1.7966\n",
      "Epoch 17/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 2.2242Epoch 00017: loss did not improve\n",
      "1577111/1577111 [==============================] - 3902s 2ms/step - loss: 2.2242\n",
      "Epoch 18/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 2.4667Epoch 00018: loss did not improve\n",
      "1577111/1577111 [==============================] - 3900s 2ms/step - loss: 2.4667\n",
      "Epoch 19/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 2.5399Epoch 00019: loss did not improve\n",
      "1577111/1577111 [==============================] - 3903s 2ms/step - loss: 2.5399\n",
      "Epoch 20/20\n",
      "1577088/1577111 [============================>.] - ETA: 0s - loss: 2.7373Epoch 00020: loss did not improve\n",
      "1577111/1577111 [==============================] - 3899s 2ms/step - loss: 2.7373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2beda5e0c18>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the best network weights\n",
    "filename = \"weights-improvement-16-1.7966.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" there are those who take mental phenomena naively, just as they would physical phenomena. this schoo \"\n",
      " aod the same as a sensetion of the semsetion of the semsetion of the semsetion of the porction of the porction of the part of the porction of the porction of the part of the porction of the porction of the porction of the part of the porction of the porction of the part of the porction of the porction of the porction of the part of the porction of the porction of the part of the porction of the porction of the porction of the part of the porction of the porction of the part of the porction of the porction of the porction of the part of the porction of the porction of the part of the porction of the porction of the porction of the part of the porction of the porction of the part of the porction of the porction of the porction of the part of the porction of the porction of the part of the porction of the porction of the porction of the part of the porction of the porction of the part of the porction of the porction of the porction of the part of the porction of the porction of the part \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# passing the test data and running the model - model dosent perform the best , can improve if we add more layers and epochs\n",
    "import sys\n",
    "p=\"There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.\"\n",
    "pattern=[char_to_int[char] for char in p.lower()]\n",
    "pattern=pattern[0:100]\n",
    "print(\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
